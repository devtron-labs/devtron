# Go Concurrency at Scale: Lessons from a Kubernetes Platform

**Duration:** 35 minutes
**Audience:** Intermediate to Advanced Go Developers
**Focus:** Deep dive into scaling Go concurrency in production

---

## Talk Outline

### 1. Introduction: The Scale Problem (3 mins)
### 2. The Evolution: From Naive to Scalable (8 mins)
### 3. Pattern 1: Worker Pool (Bounded Concurrency) (10 mins)
### 4. Pattern 2: Fan-Out/Fan-In (Parallel Aggregation) (10 mins)
### 5. Key Takeaways & Production Lessons (4 mins)

---

## 1. Introduction: The Scale Problem (3 mins)

### About Devtron

**What is Devtron?**
- Open-source Kubernetes deployment platform
- Manages 1000+ applications across 100+ clusters
- Handles 10,000+ deployments daily
- Processes thousands of CI/CD pipelines concurrently

**Our Concurrency Challenges:**
- Triggering 100+ CI pipelines simultaneously
- Fetching resources from 100+ Kubernetes clusters in parallel
- Processing thousands of webhook events
- Real-time status updates for thousands of deployments

---
### What This Talk Covers

We'll explore the **evolution of concurrency patterns** and **2 critical patterns** for scaling:

**The Evolution:**
- Approach 1: Naive (spawn goroutine per task) ‚Üí Crashes
- Approach 2: sync.WaitGroup (wait for completion) ‚Üí Still crashes at scale
- Approach 3: Worker Pool (bounded concurrency) ‚Üí Scales!

**Pattern 1: Bounded Concurrency (Worker Pools)**
- How to process 10,000 items with controlled concurrency
- Line-by-line code walkthrough
- Production metrics: Before/After

**Pattern 2: Fan-Out/Fan-In (Parallel Aggregation)**
- How to fetch from 100+ sources in parallel and safely combine results
- Why we use sync.Map vs regular map + mutex vs pre-allocated slice
- Real examples: Cluster connection, workflow status fetching

---

## 2. The Evolution: From Naive to Scalable (8 mins)

### Real-World Analogy: The Restaurant Story

**Imagine you own a restaurant that just got popular:**

**Week 1: Small restaurant (10 customers/day)**
- You cook all orders yourself
- Works perfectly fine!
- Customers are happy

**Week 2: Featured on TV (1000 customers/day)**
- You try the same approach: cook all orders yourself
- Problem: You're overwhelmed!
- Customers wait hours for food
- Kitchen runs out of ingredients
- You collapse from exhaustion

**This is exactly what happened to our system:**
- **Week 1** = Development environment (10 apps)
- **Week 2** = Production environment (1000+ apps)
- **You cooking alone** = Simple goroutines without control
- **Overwhelmed kitchen** = System crashes

Let's see how we evolved our approach, just like a restaurant evolves from a small kitchen to a professional operation.

---

### The Critical Question

**Scenario:** You need to process 1000 tasks concurrently (trigger pipelines, send emails, process files, etc.)

**Question:** How do you implement this in Go?

Let's see 3 approaches and understand why only one scales.

---

### Approach 1: Naive - Spawn Goroutine Per Task (‚ùå FAILS)

**Restaurant Analogy:**
- 1000 customers walk in
- You hire 1000 chefs on the spot
- Your kitchen has space for only 10 chefs
- Result: Chaos! Chefs bumping into each other, kitchen on fire!

**The Code:**

```go
// Approach 1: Naive - Just spawn goroutines
func ProcessTasksNaive(tasks []Task) {
    for _, task := range tasks {
        go func(t Task) {
            processTask(t)  // Do the work
        }(task)
    }
    // Function returns immediately!
}
```

**What happens:**

```
Main goroutine:
  ‚îú‚îÄ‚Üí Spawns 1000 goroutines
  ‚îî‚îÄ‚Üí Returns immediately (doesn't wait!)

Result: Function returns before any task completes!
```

**Problems:**

1. **‚ùå No waiting** - Function returns before tasks complete
2. **‚ùå No error handling** - Can't collect errors
3. **‚ùå No result collection** - Can't get results back
4. **‚ùå Unbounded concurrency** - All 1000 goroutines run at once

**When this fails:**
- 1000 tasks: Might work (if tasks are light)
- 10,000 tasks: Probably crashes (OOM, resource exhaustion)
- 100,000 tasks: Definitely crashes

**Real example:**
```go
// This is what we did initially at Devtron
for _, pipeline := range pipelines {  // 100 pipelines
    go func(p Pipeline) {
        impl.triggerCiPipeline(p)  // Each spawns ~50 more goroutines
    }(pipeline)
}
// Result: 100 √ó 50 = 5,000 goroutines ‚Üí OOM crash!
```

---

### Approach 2: sync.WaitGroup - Wait for Completion (‚ö†Ô∏è BETTER, BUT STILL FAILS AT SCALE)

**Restaurant Analogy:**
- 1000 customers walk in
- You hire 1000 chefs
- You wait for all chefs to finish before closing the restaurant
- Problem: Still 1000 chefs in a kitchen built for 10!
- Result: Better than before (you wait), but kitchen still overwhelmed!

**The Code:**

```go
// Approach 2: Use sync.WaitGroup to wait for completion
func ProcessTasksWithWaitGroup(tasks []Task) error {
    var wg sync.WaitGroup

    for _, task := range tasks {
        wg.Add(1)  // Increment counter

        go func(t Task) {
            defer wg.Done()  // Decrement counter when done
            processTask(t)
        }(task)
    }

    wg.Wait()  // Wait for all goroutines to complete
    return nil
}
```

**What happens:**

```
Main goroutine:
  ‚îú‚îÄ‚Üí Spawns 1000 goroutines (all at once)
  ‚îú‚îÄ‚Üí Waits for all to complete
  ‚îî‚îÄ‚Üí Returns after all complete

Result: Function waits, but still spawns 1000 goroutines!
```

**What's better:**
- ‚úÖ **Waits for completion** - Function doesn't return early
- ‚úÖ **Proper cleanup** - defer wg.Done() ensures counter is decremented
- ‚úÖ **Synchronization** - Main goroutine waits for all workers

**What's still wrong:**
- ‚ùå **Still unbounded** - All 1000 goroutines run simultaneously
- ‚ùå **Resource exhaustion** - Can still crash with OOM, DB exhaustion, API rate limits
- ‚ùå **No control** - Can't limit concurrency

**The key insight:**

> **sync.WaitGroup solves the "waiting" problem, but NOT the "too many goroutines" problem!**

**When this fails:**

| Tasks | Goroutines | Result |
|-------|------------|--------|
| 10 | 10 | ‚úÖ Works fine |
| 100 | 100 | ‚ö†Ô∏è Might work (depends on resources) |
| 1,000 | 1,000 | ‚ùå Likely fails (DB connections, API limits) |
| 10,000 | 10,000 | ‚ùå Definitely crashes (OOM, thrashing) |

**Real example at Devtron:**

```go
// We tried this next - added WaitGroup
var wg sync.WaitGroup

for _, pipeline := range pipelines {  // 100 pipelines
    wg.Add(1)
    go func(p Pipeline) {
        defer wg.Done()
        impl.triggerCiPipeline(p)  // Each needs DB connection
    }(pipeline)
}

wg.Wait()

// Result:
// - ‚úÖ Function waits for completion
// - ‚ùå Still spawns 100 goroutines at once
// - ‚ùå Database: "pq: sorry, too many clients already" (max 100 connections)
// - ‚ùå Kubernetes API: 429 Too Many Requests (rate limit exceeded)
```

**The problem:**
- We have 100 database connections available
- We spawn 100 goroutines
- Each goroutine tries to get a DB connection
- But other parts of the application also need connections!
- Result: Connection pool exhausted ‚Üí Crash

---

### Approach 3: Worker Pool - Bounded Concurrency (‚úÖ SCALES!)

**Restaurant Analogy:**
- 1000 customers walk in
- You hire only 5 professional chefs (your kitchen capacity)
- First 5 customers: 5 chefs cook ‚Üí customers served
- Next 5 customers: Same 5 chefs cook ‚Üí customers served
- Continue in batches of 5 until all 1000 customers served
- Result: Organized, efficient, no chaos!

**The Code:**

```go
// Approach 3: Worker Pool - Control concurrency with batching
func ProcessTasksWithWorkerPool(tasks []Task) error {
    batchSize := 5  // Only 5 goroutines at a time!

    for i := 0; i < len(tasks); {
        // Calculate batch size (last batch might be smaller)
        remainingTasks := len(tasks) - i
        if remainingTasks < batchSize {
            batchSize = remainingTasks
        }

        var wg sync.WaitGroup

        // Launch only batchSize goroutines
        for j := 0; j < batchSize; j++ {
            wg.Add(1)
            index := i + j

            go func(idx int) {
                defer wg.Done()
                processTask(tasks[idx])
            }(index)
        }

        wg.Wait()  // Wait for this batch to complete
        i += batchSize  // Move to next batch
    }

    return nil
}
```

**What happens:**

```
Main goroutine:
  ‚îú‚îÄ‚Üí Batch 1: Spawns 5 goroutines ‚Üí Waits for completion
  ‚îú‚îÄ‚Üí Batch 2: Spawns 5 goroutines ‚Üí Waits for completion
  ‚îú‚îÄ‚Üí Batch 3: Spawns 5 goroutines ‚Üí Waits for completion
  |   ... (200 batches for 1000 tasks)
  ‚îî‚îÄ‚Üí Returns after all batches complete

Result: Max 5 goroutines at any time, processes all 1000 tasks!
```

**What's better:**
- ‚úÖ **Bounded concurrency** - Never more than 5 goroutines at once
- ‚úÖ **Predictable resource usage** - Max 5 DB connections, 5 API calls
- ‚úÖ **Scales to any number** - 1000 tasks? 10,000? 1,000,000? Same max 5 goroutines
- ‚úÖ **Tunable** - Adjust batchSize based on your constraints

**When this works:**

| Tasks | Max Goroutines | Result |
|-------|----------------|--------|
| 10 | 5 | ‚úÖ Works |
| 100 | 5 | ‚úÖ Works |
| 1,000 | 5 | ‚úÖ Works |
| 10,000 | 5 | ‚úÖ Works |
| 1,000,000 | 5 | ‚úÖ Works (takes longer, but doesn't crash) |

---

### Side-by-Side Comparison

**Processing 1000 tasks:**

| Approach | Goroutines | Waits? | Scales? | Use Case |
|----------|------------|--------|---------|----------|
| **Naive** | 1000 | ‚ùå No | ‚ùå No | Never use |
| **sync.WaitGroup** | 1000 | ‚úÖ Yes | ‚ùå No | Small tasks only (< 100) |
| **Worker Pool** | 5 | ‚úÖ Yes | ‚úÖ Yes | Production at scale |

---

### The Key Difference: sync.WaitGroup vs Worker Pool

**This is the critical insight:**

> **sync.WaitGroup is a TOOL, not a PATTERN.**
>
> - **Approach 2** uses sync.WaitGroup to wait for 1000 goroutines
> - **Approach 3** uses sync.WaitGroup to wait for 5 goroutines (per batch)
>
> **Both use sync.WaitGroup, but Worker Pool limits how many goroutines exist at once!**

**Analogy:**

**Approach 2 (sync.WaitGroup only):**
- Restaurant gets 1000 orders
- Hires 1000 chefs immediately
- Manager waits for all chefs to finish
- **Problem:** Kitchen is too crowded, runs out of ingredients, chaos!

**Approach 3 (Worker Pool):**
- Restaurant gets 1000 orders
- Hires only 5 chefs
- Chefs process orders in batches
- Manager waits for each batch
- **Result:** Controlled, efficient, scalable!

---

### When to Use Each Approach

**Use Approach 2 (sync.WaitGroup only) when:**
- ‚úÖ Small number of tasks (< 50)
- ‚úÖ Tasks are very lightweight (no DB, no API calls)
- ‚úÖ No resource constraints
- ‚úÖ Example: Processing items in memory, simple calculations

**Use Approach 3 (Worker Pool) when:**
- ‚úÖ Large number of tasks (100+)
- ‚úÖ Tasks use external resources (DB, API, network)
- ‚úÖ Resource constraints exist (connection pools, rate limits)
- ‚úÖ Need predictable resource usage
- ‚úÖ **This is what you need in production!**

---

### Deep Dive: Why sync.WaitGroup?

**Question:** Why use `sync.WaitGroup` instead of channels?

**Alternative 1: Using Channels**

```go
// ‚ùå More complex, unnecessary overhead
done := make(chan bool, batchSize)

for j := 0; j < batchSize; j++ {
    go func(idx int) {
        impl.triggerCiPipeline(pipelines[idx])
        done <- true  // Send completion signal
    }(i + j)
}

// Wait for all goroutines
for j := 0; j < batchSize; j++ {
    <-done  // Receive completion signal
}
```

**Problems with channels:**
1. Need to create buffered channel (size = batchSize)
2. Need to send signal after work
3. Need to receive exactly batchSize times
4. More allocations (channel creation)
5. More complex to read

**Alternative 2: Using sync.WaitGroup**

```go
// ‚úÖ Simpler, clearer intent
var wg sync.WaitGroup

for j := 0; j < batchSize; j++ {
    wg.Add(1)
    go func(idx int) {
        defer wg.Done()
        impl.triggerCiPipeline(pipelines[idx])
    }(i + j)
}

wg.Wait()  // Clear intent: "wait for group"
```

**Why WaitGroup wins:**
1. ‚úÖ Clear semantic intent: "wait for group of goroutines"
2. ‚úÖ Less code, easier to read
3. ‚úÖ No channel allocation overhead
4. ‚úÖ defer wg.Done() is idiomatic
5. ‚úÖ No need to count receives

**When to use channels instead:**
- When you need to pass data between goroutines
- When you need to implement producer-consumer pattern
- When you need select with multiple channels

**Our case:** We just need to wait for completion ‚Üí WaitGroup is perfect

---

### Key Takeaways: Worker Pool Pattern

**What we learned:**
1. ‚úÖ **Batching controls concurrency** - Only N goroutines at a time
2. ‚úÖ **sync.WaitGroup for waiting** - Simpler than channels for this use case
3. ‚úÖ **Configurable batch size** - Tune based on resource limits
4. ‚úÖ **Avoid loop variable capture** - Pass index as parameter

**When to use Worker Pool:**
- ‚úÖ Processing large number of independent tasks
- ‚úÖ Resource constraints exist (DB connections, API limits)
- ‚úÖ Need predictable, controlled resource usage
- ‚úÖ Tasks are I/O-bound (network, database, file operations)

**Production Impact at Devtron:**
- **Before:** System crashes with 100+ concurrent triggers
- **After:** Stable processing of 1000+ triggers with batch size = 5
- **Result:** 100% success rate, predictable resource usage

---

## 4. Pattern 2: Fan-Out/Fan-In (Parallel Aggregation) (10 mins)

### What is Fan-Out/Fan-In?

**Fan-Out:** Distribute work to multiple goroutines running in parallel
**Fan-In:** Collect results from all goroutines into a single place

**The Pattern:**
```
Input ‚Üí Fan-Out ‚Üí [Worker 1, Worker 2, Worker 3, ...] ‚Üí Fan-In ‚Üí Combined Result
```

**Use Cases:**
- Fetching data from multiple sources (databases, APIs, clusters)
- Parallel processing with result aggregation
- Scatter-gather pattern

**Key Difference from Worker Pool:**
- **Worker Pool:** Process many tasks in controlled batches (bounded concurrency)
- **Fan-Out/Fan-In:** Process N tasks in parallel, collect N results (often unbounded, but N is small)

---

### Real-World Analogy: The Library Research

**Imagine you're a student researching for a paper:**

You need information from 2 different sections of the library:
- **Section A:** History books (5 minutes to walk there, find book, and return)
- **Section B:** Science books (5 minutes to walk there, find book, and return)

**Option 1: Sequential (You do it alone)**
- Walk to Section A, get history book ‚Üí 5 minutes
- Walk to Section B, get science book ‚Üí 5 minutes
- **Total time:** 10 minutes

**Option 2: Parallel (You and your friend)**
- You walk to Section A for history book ‚Üí 5 minutes
- Your friend walks to Section B for science book ‚Üí 5 minutes (at the same time!)
- You both meet back at the table
- **Total time:** 5 minutes (50% faster!)

**This is exactly what happens in our system:**
- **Section A** = CI workflow status query
- **Section B** = CD workflow status query
- **You and your friend** = 2 goroutines running in parallel
- **Meeting back at table** = sync.WaitGroup waiting for both to complete

---
### Real-World Example : Fetching Workflow Status

**Context:** User opens the application dashboard and needs to see CI and CD workflow status.

**The Problem:**
- Need to fetch CI workflow status (calls database, ~500ms)
- Need to fetch CD workflow status (calls database, ~500ms)
- Sequential: 500ms + 500ms = 1000ms (too slow!)
- User sees loading spinner for 1 second

**The Solution:** Fetch both in parallel! (Like the library example)

---

### Code Walkthrough: Parallel Status Fetching

**File:** `api/restHandler/app/pipeline/configure/PipelineConfigRestHandler.go`
**Function:** `FetchWorkflowStatus`

**Real production code from Devtron:**

```go
// This function is called when user opens the application dashboard
// It needs to show both CI and CD workflow status

func (handler *PipelineConfigRestHandlerImpl) FetchWorkflowStatus(
    w http.ResponseWriter,
    r *http.Request,
) {
    // Get appId from request
    vars := mux.Vars(r)
    appId, _ := strconv.Atoi(vars["app-id"])

    // STEP 1: Declare variables to store results
    // Why separate variables: Each goroutine will write to its own variable
    var ciWorkflowStatus []*pipelineConfig.CiWorkflowStatus
    var cdWorkflowStatus []*pipelineConfig.CdWorkflowStatus
    var err error
    var err1 error

    // STEP 2: Create WaitGroup for 2 goroutines
    // Why 2: We're launching exactly 2 goroutines (CI and CD)
    wg := sync.WaitGroup{}
    wg.Add(2)  // Increment counter by 2

    // STEP 3: Launch goroutine to fetch CI status
    go func() {
        // STEP 3a: Ensure Done() is called
        // Why defer: Guarantees execution even if panic
        defer wg.Done()

        // STEP 3b: Fetch CI workflow status
        // This function:
        // - Queries ci_workflow table
        // - Joins with ci_pipeline table
        // - Aggregates status for all CI pipelines
        // Takes ~500ms
        ciWorkflowStatus, err = handler.ciHandler.FetchCiStatusForTriggerView(appId)
    }()

    // STEP 4: Launch goroutine to fetch CD status
    go func() {
        // STEP 4a: Ensure Done() is called
        defer wg.Done()

        // STEP 4b: Fetch CD workflow status
        // This function:
        // - Queries cd_workflow table
        // - Joins with cd_pipeline table
        // - Aggregates status for all CD pipelines
        // Takes ~500ms
        cdWorkflowStatus, err1 = handler.cdHandler.FetchAppWorkflowStatusForTriggerView(appId)
    }()

    // STEP 5: Wait for both goroutines to complete
    // Why Wait(): We need both results before responding to user
    // At this point, both goroutines are running in parallel
    wg.Wait()

    // STEP 6: Check for errors
    if err != nil {
        handler.Logger.Errorw("service err, FetchAppWorkflowStatusForTriggerView",
            "err", err, "appId", appId)
        common.WriteJsonResp(w, err, nil, http.StatusInternalServerError)
        return
    }

    // STEP 7: Combine results and return
    triggerWorkflowStatus := pipelineConfig.TriggerWorkflowStatus{
        CiWorkflowStatus: ciWorkflowStatus,
        CdWorkflowStatus: cdWorkflowStatus,
    }

    common.WriteJsonResp(w, nil, triggerWorkflowStatus, http.StatusOK)
}
```

**Results:**
- **Before (Sequential):** 500ms + 500ms = 1000ms
- **After (Parallel):** max(500ms, 500ms) = 500ms
- **Speedup:** 2x faster!
- **User experience:** Dashboard loads in half the time

**Why this works:**
- CI and CD fetches are independent (no shared state)
- Both can run simultaneously
- We wait for both to complete before responding
- Simple, effective, scales well

---
### Key Takeaways: Fan-Out/Fan-In Pattern

**What we learned:**
1. ‚úÖ **Fan-Out:** Launch multiple goroutines in parallel for independent tasks
2. ‚úÖ **Fan-In:** Collect results safely using sync.Map or pre-allocated slice
3. ‚úÖ **sync.Map for dynamic keys** - When you don't know keys in advance
4. ‚úÖ **Pre-allocated slice for indexed results** - When you know indices
5. ‚úÖ **atomic operations for counters** - Fast, lock-free increments

**When to use Fan-Out/Fan-In:**
- ‚úÖ Fetching from multiple independent sources (APIs, databases, clusters)
- ‚úÖ Need to combine/aggregate results
- ‚úÖ Can tolerate partial failures
- ‚úÖ N is relatively small (< 1000) or use with batching

**Production Impact at Devtron:**
- **Workflow status:** 1000ms ‚Üí 500ms (2x faster)
- **Cluster connection:** 20s ‚Üí 300ms (66x faster)
- **Resource fetching:** 5s ‚Üí 1s (5x faster)

---

## 5. Key Takeaways & Production Lessons (4 mins)

### The Evolution Summary

**We covered 3 approaches to concurrent task processing:**

| Approach | Code Pattern | Goroutines | Waits? | Scales? | When to Use |
|----------|--------------|------------|--------|---------|-------------|
| **1. Naive** | `go func()` in loop | N (all tasks) | ‚ùå No | ‚ùå No | Never in production |
| **2. sync.WaitGroup** | `wg.Add(1)` + `go func()` + `wg.Wait()` | N (all tasks) | ‚úÖ Yes | ‚ùå No | Small N (< 50), no resources |
| **3. Worker Pool** | Batching + `wg.Add(1)` + `go func()` + `wg.Wait()` | B (batch size) | ‚úÖ Yes | ‚úÖ Yes | **Production at scale** |

**Key Insight:**
> **sync.WaitGroup is a synchronization tool, not a scaling solution.**
>
> - Approach 2 uses it to wait for N goroutines (unbounded)
> - Approach 3 uses it to wait for B goroutines per batch (bounded)
>
> **Worker Pool = Batching + sync.WaitGroup**

---

### Pattern Summary

**Pattern 1: Worker Pool (Bounded Concurrency)**

**When to use:**
- Processing large number of items (1000+)
- External resource limits (database, API)
- Need predictable resource usage

**Key techniques:**
- Batch processing with fixed size
- sync.WaitGroup for coordination
- Pass loop variables correctly

**Production impact:**
- Crash ‚Üí Stable
- 0% success ‚Üí 100% success
- Predictable resource usage

---

**Pattern 2: Fan-Out/Fan-In (Parallel Aggregation)**

**When to use:**
- Fetching from multiple independent sources
- Need to combine results
- Can tolerate partial failures

**Key techniques:**
- sync.Map for thread-safe result collection
- Pre-allocated slice for indexed results
- atomic operations for counters
- Error handling per goroutine

**Production impact:**
- 20s ‚Üí 300ms (66x faster)
- Better user experience
- Graceful degradation

---

### Best Practices from Production

**1. Always Use defer wg.Done()**
```go
// ‚úÖ ALWAYS
go func() {
    defer wg.Done()  // Guarantees execution
    doWork()
}()
```

**Why:** If doWork() panics, Done() is still called. Without defer, deadlock.

---

**2. Pass Loop Variables Correctly**
```go
// ‚úÖ CORRECT
for i, item := range items {
    go func(idx int, it Item) {
        process(idx, it)
    }(i, item)
}
```

**Why:** Avoid loop variable capture bug.

---

**3. Choose the Right Approach - Decision Tree**

```
Do you need to process multiple tasks concurrently?
‚îÇ
‚îú‚îÄ No ‚Üí Just use sequential processing
‚îÇ
‚îî‚îÄ Yes ‚Üí How many tasks?
    ‚îÇ
    ‚îú‚îÄ Small (< 50) AND no external resources (DB, API)
    ‚îÇ   ‚Üí Use sync.WaitGroup (Approach 2)
    ‚îÇ   ‚Üí Simple, fast, no need for batching
    ‚îÇ
    ‚îî‚îÄ Large (100+) OR uses external resources
        ‚Üí Use Worker Pool (Approach 3)
        ‚Üí Bounded concurrency, predictable resources

        How to choose batch size?
        ‚îÇ
        ‚îú‚îÄ External API rate limit exists?
        ‚îÇ   ‚Üí batch_size = rate_limit / calls_per_task
        ‚îÇ
        ‚îú‚îÄ Database connection pool limit?
        ‚îÇ   ‚Üí batch_size = available_connections / 2
        ‚îÇ
        ‚îú‚îÄ Memory constrained?
        ‚îÇ   ‚Üí batch_size = available_memory / memory_per_goroutine
        ‚îÇ
        ‚îî‚îÄ No constraints?
            ‚Üí Start with batch_size = num_cpu_cores
            ‚Üí Monitor and tune based on metrics
```

**Examples:**

| Scenario | Tasks | Resources | Approach | Batch Size |
|----------|-------|-----------|----------|------------|
| Process 20 files in memory | 20 | None | sync.WaitGroup | N/A |
| Send 100 emails via API (10 QPS limit) | 100 | API | Worker Pool | 5 |
| Trigger 1000 CI pipelines (DB + K8s API) | 1000 | DB + API | Worker Pool | 5-10 |
| Fetch from 100 K8s clusters | 100 | Network | Fan-Out/Fan-In | 100 (all) |

---

### Final Thoughts

**Goroutines are cheap, but not free:**
- Each goroutine: ~2KB stack + heap allocations
- Context switching overhead
- Resource contention (database, API, memory)

**Scale requires discipline:**
- Bounded concurrency (worker pools)
- Safe state management (sync.Map, atomic)
- Proper error handling
- Monitoring and tuning

**Production lessons:**
- Start simple, measure, optimize
- Stability > Speed
- Monitor everything
- Plan for failures

---

## Questions?

**Resources:**
- Devtron GitHub: https://github.com/devtron-labs/devtron
- Go Concurrency Patterns: https://go.dev/blog/pipelines
- sync package: https://pkg.go.dev/sync
- atomic package: https://pkg.go.dev/sync/atomic

**Thank you!** üöÄ


