# Go Concurrency at Scale: Lessons from a Kubernetes Platform
## Real-World Patterns from Devtron

**Duration:** 25 minutes  
**Audience:** Intermediate to Advanced Go Developers

---

## üìã Talk Outline

### 1. Introduction (2 mins)
### 2. Understanding sync.WaitGroup - The Foundation (5 mins)
### 3. Worker Pools Pattern (7 mins)
### 4. Fan-Out/Fan-In Pattern (6 mins)
### 5. Graceful Shutdown with Context (4 mins)
### 6. Q&A (1 min)

---

## 1. Introduction (2 mins)

### Why Beyond Basic Goroutines?

**Real-World Scenario:**
Imagine you're running a restaurant kitchen üç≥

**Bad Approach (Unlimited Goroutines):**
```go
// ‚ùå Hire unlimited chefs for every order
for _, order := range orders {
    go cookOrder(order)  // 1000 orders = 1000 chefs!
}
```

**Problems:**
- üî• Kitchen overcrowded (resource exhaustion)
- üî• No coordination (orders mixed up)
- üî• No one checks if cooking failed (silent failures)
- üî• Can't close kitchen gracefully (data loss)

**What We'll Learn:**
- How to control the number of "workers" (chefs)
- How to coordinate their work (sync.WaitGroup)
- How to handle errors properly
- How to shutdown gracefully
- Real examples from Devtron (Kubernetes platform handling 10,000+ deployments daily)

---

## 2. Understanding sync.WaitGroup - The Foundation (5 mins)

### What is sync.WaitGroup?

**Real-World Analogy: Restaurant Manager üë®‚Äçüíº**

Imagine you're a restaurant manager:
- You assign 5 chefs to cook 5 dishes
- You need to wait until ALL dishes are ready before serving
- How do you know when everyone is done?

**This is exactly what sync.WaitGroup does!**

---

### Visual Diagram: How WaitGroup Works

```
Manager (Main Goroutine)
   |
   |-- wg.Add(1) --> Chef 1 starts cooking üßë‚Äçüç≥
   |-- wg.Add(1) --> Chef 2 starts cooking üßë‚Äçüç≥
   |-- wg.Add(1) --> Chef 3 starts cooking üßë‚Äçüç≥
   |
   |-- wg.Wait() --> ‚è≥ Manager waits...
   |
   |   Chef 1: wg.Done() ‚úÖ (dish ready)
   |   Chef 2: wg.Done() ‚úÖ (dish ready)
   |   Chef 3: wg.Done() ‚úÖ (dish ready)
   |
   |-- All done! Continue serving üçΩÔ∏è
```

---

### Simple Code Example

```go
func main() {
    var wg sync.WaitGroup

    // We have 3 tasks
    tasks := []string{"Cook pasta", "Make salad", "Bake bread"}

    for _, task := range tasks {
        wg.Add(1)  // üìù Tell manager: "One more chef is working"

        go func(taskName string) {
            defer wg.Done()  // ‚úÖ Tell manager: "I'm done!"

            fmt.Println("Starting:", taskName)
            time.Sleep(1 * time.Second)  // Simulate work
            fmt.Println("Finished:", taskName)
        }(task)
    }

    wg.Wait()  // ‚è≥ Wait for all chefs to finish
    fmt.Println("All tasks complete! Ready to serve!")
}
```

**Output:**
```
Starting: Cook pasta
Starting: Make salad
Starting: Bake bread
Finished: Cook pasta
Finished: Make salad
Finished: Bake bread
All tasks complete! Ready to serve!
```

---

### Why sync.WaitGroup Instead of Other Options?

**Q: Why not just use channels?**
```go
// ‚ùå More complex for simple "wait for all" scenario
done := make(chan bool, 3)
for _, task := range tasks {
    go func() {
        doWork()
        done <- true  // Need to send signal
    }()
}
// Need to receive exactly 3 times
for i := 0; i < 3; i++ {
    <-done
}
```

**A: WaitGroup is simpler when you just need to "wait for all to complete"**
- ‚úÖ No need to create channels
- ‚úÖ No need to count receives
- ‚úÖ Clear intent: "wait for group"
- ‚úÖ Less code, easier to read

**Q: Why not use sync.Mutex?**

**A: Different purpose!**
- **Mutex** = Lock/unlock access to shared data (like a bathroom lock üö™)
- **WaitGroup** = Wait for multiple tasks to complete (like waiting for all chefs)

**Q: When to use WaitGroup?**
- ‚úÖ You have multiple goroutines doing work
- ‚úÖ You need to wait for ALL of them to finish
- ‚úÖ You don't need to collect results (just wait)

---

## 3. Worker Pools Pattern (7 mins)

### The Problem: Too Many Goroutines

**Bad Code:**
```go
// ‚ùå Processing 1000 items = 1000 goroutines!
for _, item := range items {
    go processItem(item)
}
// No control, no waiting, chaos!
```

**What happens:**
- üí• 1000 goroutines created instantly
- üí• System runs out of memory
- üí• Database connections exhausted
- üí• Application crashes

---

### Solution: Worker Pool (Controlled Concurrency)

**Real-World Analogy: Assembly Line üè≠**

Instead of hiring 1000 workers:
- Hire only 5 workers (batch size)
- Give them 200 items each
- Wait for batch to finish
- Start next batch

**Visual Diagram:**
```
Items: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
Batch Size: 5

Batch 1:
  Worker 1 ‚Üí Item 1  ‚úÖ
  Worker 2 ‚Üí Item 2  ‚úÖ
  Worker 3 ‚Üí Item 3  ‚úÖ
  Worker 4 ‚Üí Item 4  ‚úÖ
  Worker 5 ‚Üí Item 5  ‚úÖ
  [Wait for all to finish]

Batch 2:
  Worker 1 ‚Üí Item 6  ‚úÖ
  Worker 2 ‚Üí Item 7  ‚úÖ
  Worker 3 ‚Üí Item 8  ‚úÖ
  Worker 4 ‚Üí Item 9  ‚úÖ
  Worker 5 ‚Üí Item 10 ‚úÖ
  [Wait for all to finish]

Batch 3:
  Worker 1 ‚Üí Item 11 ‚úÖ
  Worker 2 ‚Üí Item 12 ‚úÖ
  Worker 3 ‚Üí Item 13 ‚úÖ
  Worker 4 ‚Üí Item 14 ‚úÖ
  Worker 5 ‚Üí Item 15 ‚úÖ
  [Done!]
```

---

### Simple Code Example

```go
func ProcessItemsInBatches(items []string) {
    batchSize := 5  // Only 5 workers at a time
    totalItems := len(items)

    for i := 0; i < totalItems; {
        // Calculate how many items in this batch
        remainingItems := totalItems - i
        if remainingItems < batchSize {
            batchSize = remainingItems  // Last batch might be smaller
        }

        var wg sync.WaitGroup

        // Start workers for this batch
        for j := 0; j < batchSize; j++ {
            wg.Add(1)
            index := i + j

            go func(idx int) {
                defer wg.Done()
                processItem(items[idx])  // Do the work
            }(index)
        }

        wg.Wait()  // Wait for this batch to complete
        i += batchSize  // Move to next batch
    }
}
```

**Key Points:**
1. **`batchSize := 5`** - Only 5 goroutines at a time (not 1000!)
2. **`wg.Add(1)`** - Track each worker
3. **`defer wg.Done()`** - Worker signals completion
4. **`wg.Wait()`** - Wait for batch before starting next
5. **`i += batchSize`** - Move to next batch

---

### Real Example from Devtron

**Scenario:** After CI build succeeds, trigger CD deployments

**File:** `pkg/workflow/dag/WorkflowDagExecutor.go`

**Simplified Code:**
```go
// We have 100 CI artifacts that need CD deployment
artifacts := []CIArtifact{...}  // 100 items
batchSize := 5  // Only 5 concurrent deployments

for i := 0; i < len(artifacts); {
    remainingBatch := len(artifacts) - i
    if remainingBatch < batchSize {
        batchSize = remainingBatch
    }

    var wg sync.WaitGroup

    for j := 0; j < batchSize; j++ {
        wg.Add(1)
        index := i + j

        go func(idx int) {
            defer wg.Done()
            artifact := artifacts[idx]

            // Trigger CD deployment for this artifact
            err := triggerDeployment(artifact)
            if err != nil {
                log.Error("Deployment failed", artifact.ID, err)
            }
        }(index)
    }

    wg.Wait()  // Wait for batch to complete
    i += batchSize
}
```

**Why This Works:**
- ‚úÖ **Before:** 100 concurrent deployments ‚Üí System crash üí•
- ‚úÖ **After:** 5 concurrent deployments ‚Üí Stable, predictable
- ‚úÖ **Performance:** Still fast (20 batches √ó 2 seconds = 40 seconds)
- ‚úÖ **Reliability:** No resource exhaustion

---

## 4. Fan-Out/Fan-In Pattern (6 mins)

### What is Fan-Out/Fan-In?

**Real-World Analogy: Research Team üìö**

You're writing a report and need information from 3 different sources:
- Database statistics
- API response times
- User feedback

**Sequential (Slow):**
```
You ‚Üí Get DB stats (2 sec) ‚Üí Get API times (2 sec) ‚Üí Get feedback (2 sec)
Total: 6 seconds ‚è±Ô∏è
```

**Parallel (Fast - Fan-Out/Fan-In):**
```
        ‚îå‚îÄ‚Üí Person 1: Get DB stats (2 sec) ‚îÄ‚îê
You ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚Üí Person 2: Get API times (2 sec) ‚îÄ‚îº‚îÄ‚Üí Combine results
        ‚îî‚îÄ‚Üí Person 3: Get feedback (2 sec) ‚îÄ‚îÄ‚îò

Total: 2 seconds ‚è±Ô∏è (3x faster!)
```

---

### Visual Diagram: Fan-Out/Fan-In

```
Main Goroutine
      |
      |--- FAN-OUT (Split work) --->
      |
      ‚îú‚îÄ‚Üí Goroutine 1: Fetch CI status    ‚îÄ‚îê
      |                                     |
      ‚îú‚îÄ‚Üí Goroutine 2: Fetch CD status    ‚îÄ‚î§
      |                                     |
      ‚îú‚îÄ‚Üí Goroutine 3: Fetch user info    ‚îÄ‚î§
      |                                     |
      |<-- FAN-IN (Collect results) -------‚îò
      |
      |--- Combine all results --->
      |
    Continue
```

---

### Simple Code Example

```go
func FetchDashboardData(userID int) DashboardData {
    var ciStatus []CIStatus
    var cdStatus []CDStatus
    var userInfo UserInfo

    var wg sync.WaitGroup
    wg.Add(3)  // We're launching 3 goroutines

    // FAN-OUT: Launch parallel tasks
    go func() {
        defer wg.Done()
        ciStatus = fetchCIStatus(userID)  // Takes 2 seconds
    }()

    go func() {
        defer wg.Done()
        cdStatus = fetchCDStatus(userID)  // Takes 2 seconds
    }()

    go func() {
        defer wg.Done()
        userInfo = fetchUserInfo(userID)  // Takes 2 seconds
    }()

    // FAN-IN: Wait for all to complete
    wg.Wait()

    // Combine results
    return DashboardData{
        CI:   ciStatus,
        CD:   cdStatus,
        User: userInfo,
    }
}
```

**Performance:**
- ‚ùå Sequential: 2 + 2 + 2 = **6 seconds**
- ‚úÖ Parallel: max(2, 2, 2) = **2 seconds** (3x faster!)

---

### Real Example from Devtron

**Scenario:** User opens deployment dashboard, needs CI + CD status

**File:** `api/restHandler/app/pipeline/configure/PipelineConfigRestHandler.go`

**Simplified Code:**
```go
func FetchWorkflowStatus(appID int) WorkflowStatus {
    var ciStatus []CIWorkflow
    var cdStatus []CDWorkflow
    var err1, err2 error

    var wg sync.WaitGroup
    wg.Add(2)

    // FAN-OUT: Fetch CI and CD status in parallel
    go func() {
        defer wg.Done()
        ciStatus, err1 = fetchCIStatus(appID)
    }()

    go func() {
        defer wg.Done()
        cdStatus, err2 = fetchCDStatus(appID)
    }()

    // FAN-IN: Wait for both
    wg.Wait()

    // Handle errors
    if err1 != nil || err2 != nil {
        log.Error("Failed to fetch status")
    }

    // Combine and return
    return WorkflowStatus{
        CI: ciStatus,
        CD: cdStatus,
    }
}
```

**Why This Matters:**
- ‚úÖ **Before:** Fetch CI (500ms) + Fetch CD (500ms) = 1000ms
- ‚úÖ **After:** Fetch both in parallel = 500ms (2x faster!)
- ‚úÖ **User Experience:** Dashboard loads faster
- ‚úÖ **Scale:** With 1000 users, saves 500 seconds of total wait time!
        CdWorkflowStatus: cdWorkflowStatus,
    }
    
    common.WriteJsonResp(w, err, triggerWorkflowStatus, http.StatusOK)
}
```

---

### Why sync.Map Instead of Regular Map?

**Q: Why use `sync.Map` for collecting results?**

**Problem with Regular Map:**
```go
// ‚ùå DANGER: Race condition!
results := make(map[int]string)

for i := 0; i < 10; i++ {
    go func(id int) {
        results[id] = fetchData(id)  // Multiple goroutines writing!
    }(i)
}
// CRASH: concurrent map writes
```

**Solution 1: Mutex (More code)**
```go
results := make(map[int]string)
var mutex sync.Mutex

for i := 0; i < 10; i++ {
    go func(id int) {
        data := fetchData(id)
        mutex.Lock()
        results[id] = data
        mutex.Unlock()
    }(i)
}
```

**Solution 2: sync.Map (Built-in thread-safety)**
```go
// ‚úÖ Thread-safe by default
var results sync.Map

for i := 0; i < 10; i++ {
    go func(id int) {
        data := fetchData(id)
        results.Store(id, data)  // Safe!
    }(i)
}
```

**When to use sync.Map:**
- ‚úÖ Multiple goroutines writing to map
- ‚úÖ Don't want to manage mutex manually
- ‚úÖ Read-heavy workloads (sync.Map is optimized for this)

---

## 5. Graceful Shutdown with Context (4 mins)

### What is Context?

**Real-World Analogy: Canceling a Food Order üçï**

You order pizza online:
- Delivery time: 30 minutes
- But you need to leave in 10 minutes

**Without Context:**
```
You leave ‚Üí Pizza still being made ‚Üí Wasted resources
```

**With Context:**
```
You cancel order ‚Üí Kitchen stops making pizza ‚Üí Resources saved
```

**This is what `context.Context` does in Go!**

---

### Visual Diagram: Context Cancellation

```
HTTP Request arrives
      |
      |-- Create context with timeout (30 sec)
      |
      ‚îú‚îÄ‚Üí Start database query (uses context)
      |
      ‚îú‚îÄ‚Üí Start API call (uses context)
      |
      |   User closes browser! ‚ùå
      |
      |-- Context canceled!
      |
      ‚îú‚îÄ‚Üí Database query stops ‚úÖ
      |
      ‚îú‚îÄ‚Üí API call stops ‚úÖ
      |
    Resources freed!
```

---

### Simple Code Example

```go
func ProcessRequest(w http.ResponseWriter, r *http.Request) {
    // Create context with 5-second timeout
    ctx, cancel := context.WithTimeout(r.Context(), 5*time.Second)
    defer cancel()  // Always call cancel to free resources

    // Start long-running operation
    result := make(chan string, 1)

    go func() {
        // Simulate slow database query
        time.Sleep(10 * time.Second)
        result <- "Data from database"
    }()

    // Wait for result OR context cancellation
    select {
    case data := <-result:
        fmt.Fprintf(w, "Success: %s", data)
    case <-ctx.Done():
        // Context canceled (timeout or user disconnected)
        fmt.Fprintf(w, "Request canceled: %v", ctx.Err())
    }
}
```

**What happens:**
- If query finishes in < 5 sec ‚Üí Return data ‚úÖ
- If query takes > 5 sec ‚Üí Timeout, return error ‚ùå
- If user closes browser ‚Üí Cancel immediately ‚ùå

---

### Why Context Instead of Just Channels?

**Q: Why not just use channels for cancellation?**

**Without Context (Manual cancellation):**
```go
// ‚ùå Need to pass cancel channel everywhere
func ProcessData(cancel <-chan bool) {
    select {
    case <-cancel:
        return
    default:
        // do work
    }

    // Need to pass cancel to every function
    fetchFromDB(cancel)
    callAPI(cancel)
}
```

**With Context (Automatic propagation):**
```go
// ‚úÖ Context automatically propagates
func ProcessData(ctx context.Context) {
    // Context automatically checked
    data := fetchFromDB(ctx)
    result := callAPI(ctx, data)
    return result
}
```

**Benefits of Context:**
- ‚úÖ Automatic cancellation propagation
- ‚úÖ Built-in timeout support
- ‚úÖ Standard library integration
- ‚úÖ Less boilerplate code

---

### Real Example from Devtron

**Scenario:** User creates a Kubernetes cluster connection

**File:** `api/cluster/ClusterRestHandler.go`

**Simplified Code:**
```go
func SaveCluster(w http.ResponseWriter, r *http.Request) {
    // Get context from HTTP request
    ctx := r.Context()

    // If user closes browser, ctx will be canceled

    // Parse request
    var cluster ClusterBean
    json.NewDecoder(r.Body).Decode(&cluster)

    // Save to database (respects context)
    err := saveToDatabase(ctx, cluster)
    if err != nil {
        if ctx.Err() == context.Canceled {
            // User disconnected, don't bother responding
            return
        }
        http.Error(w, err.Error(), 500)
        return
    }

    // Test cluster connection (respects context)
    err = testClusterConnection(ctx, cluster)
    if err != nil {
        if ctx.Err() == context.Canceled {
            // User disconnected
            return
        }
        http.Error(w, err.Error(), 500)
        return
    }

    json.NewEncoder(w).Encode(cluster)
}
```

**Why This Matters:**
- ‚úÖ User closes browser ‚Üí Stop expensive K8s API calls
- ‚úÖ Saves server resources
- ‚úÖ Prevents orphaned operations
---

## 6. Summary & Key Takeaways (1 min)

### Patterns We Learned

**1. sync.WaitGroup - The Foundation**
- Wait for multiple goroutines to complete
- Simple: Add(1), Done(), Wait()
- Use when: You need to wait for all tasks

**2. Worker Pools - Control Concurrency**
- Limit number of concurrent goroutines
- Process items in batches
- Use when: You have many items to process

**3. Fan-Out/Fan-In - Parallel Processing**
- Split work across goroutines
- Collect results back
- Use when: Independent tasks that can run in parallel

**4. Context - Graceful Cancellation**
- Propagate cancellation signals
- Handle timeouts
- Use when: Long-running operations that might need to stop

---

### Real-World Impact at Devtron

| Pattern | Use Case | Improvement |
|---------|----------|-------------|
| Worker Pool | CI auto-trigger (100 pipelines) | ‚ùå Crash ‚Üí ‚úÖ 2 seconds |
| Fan-Out/Fan-In | Workflow status fetch | 500ms ‚Üí 300ms (40% faster) |
| Context | HTTP request handling | Saves resources on disconnect |

---

### When to Use Each Pattern

**Worker Pool:**
```
‚úÖ Processing large datasets
‚úÖ Batch operations
‚úÖ Controlling resource usage
```

**Fan-Out/Fan-In:**
```
‚úÖ Independent parallel operations
‚úÖ Aggregating results from multiple sources
‚úÖ Reducing total latency
```

**Context:**
```
‚úÖ HTTP request handlers
‚úÖ Long-running operations
‚úÖ Graceful shutdown
‚úÖ Timeout handling
```

---

### Common Mistakes to Avoid

**‚ùå Forgetting defer wg.Done()**
```go
go func() {
    // If this panics, wg.Done() never called!
    doWork()
    wg.Done()  // ‚ùå BAD
}()
```

**‚úÖ Always use defer**
```go
go func() {
    defer wg.Done()  // ‚úÖ GOOD - always called
    doWork()
}()
```

**‚ùå Not passing loop variable correctly**
```go
for i := 0; i < 10; i++ {
    go func() {
        process(i)  // ‚ùå BAD - all goroutines see same i
    }()
}
```

**‚úÖ Pass as parameter**
```go
for i := 0; i < 10; i++ {
    go func(index int) {
        process(index)  // ‚úÖ GOOD - each gets own copy
    }(i)
}
```

**‚ùå Forgetting to call context cancel**
```go
ctx, cancel := context.WithTimeout(parent, 5*time.Second)
// ‚ùå BAD - resource leak if function returns early
doWork(ctx)
```

**‚úÖ Always defer cancel**
```go
ctx, cancel := context.WithTimeout(parent, 5*time.Second)
defer cancel()  // ‚úÖ GOOD - always cleanup
doWork(ctx)
```

---

## 7. Q&A (1 min)

### Potential Questions to Prepare For:

**Q: When should I use channels vs WaitGroup?**
A: Use WaitGroup when you just need to wait for completion. Use channels when you need to pass data between goroutines.

**Q: How do I choose the right batch size?**
A: Start with 5-10, then benchmark. Consider:
- Available resources (CPU, memory)
- External API rate limits
- Database connection pool size

**Q: What if one goroutine panics?**
A: Use `defer recover()` inside goroutines to handle panics gracefully.

**Q: Can I nest WaitGroups?**
A: Yes! Each function can have its own WaitGroup.

**Q: How do I collect errors from multiple goroutines?**
A: Use channels, sync.Map, or errgroup package.

---

## Thank You!

**Resources:**
- Devtron GitHub: github.com/devtron-labs/devtron
- Go Concurrency Patterns: golang.org/doc/effective_go#concurrency
- Context Package: pkg.go.dev/context

**Questions?** üôã‚Äç‚ôÇÔ∏è